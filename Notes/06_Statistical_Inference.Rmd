---
title: "Statistical Inference"
author: "Sébastien Plat"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, echo=FALSE,warning=FALSE,message=FALSE}
a4width<- 8.3
a4height<- 11.7
source("..\\Courses projects\\multiplot.R")
library(ggplot2)
library(dplyr)
library(reshape2)
library(manipulate) 
lambda <- 0.2
expected_mean <- 1/lambda     
```

\pagebreak

# Statistical Inference 

## Definition 

Statistical inference is the process of drawing general conclusions about a population by:

+ using noisy **statistical data** (samples)
+ quantifying the **uncertainty** associated with those conclusions

_Note: The uncertainty could arise from incomplete or bad data._

## Terms

First, a **statistic** (singular) is a number computed from a **sample of data**. We use statistics to infer information about a population.

Second, a **random variable** is an **outcome from an experiment**. 
Deterministic processes, such as computing means or variances, applied to random variables, produce additional random variables which have their own distributions. 

Random variables are said to be iid if they are **independent and identically distributed**. 

+ **Independent** means "statistically unrelated from one another"
+ **Identically distributed** means that "all have been drawn from the same population distribution"

IID random variables are the default model for random samples; many of the important theories assume that variables are iid.
We'll usually assume that our samples are random and that variables are iid.

\  

Finally, there are two broad flavors of inference:

+ **Frequency**, which uses "long run proportion of times an event occurs in iid repetitions." 
+ **Bayesian**, in which the probability estimate for a hypothesis is updated as additional evidence is acquired. 

Both flavors require an understanding of probability: quantifying the likelihood of particular events occurring.
For a given experiment, the probability of a particular outcome is the number of ways that outcome can occur divided by all the possible outcomes.

_Please refer to **SI01 - probability.md** to know more about probabilities._

\pagebreak


# Exponential Distribution

_Please refer to **SI03 - common distributions.md** to know more about Binomial, Gaussian and Poisson distributions._

## Definition

According to [Wikipedia](https://en.wikipedia.org/wiki/Exponential_distribution):

> The exponential distribution [...] is the probability distribution 
> that describes the time between events in a Poisson process, i.e. a process 
> in which events occur continuously and independently at a constant **average rate $\lambda$**.

+ Its mean is 1/ $\lambda$
+ Its standard deviation is also 1/	$\lambda$

In our example, we will use **$\lambda$=0.2**. 

## Samples mean & variance

According to the Law of Large Numbers, samples mean & sample variance are **consistent estimators** of the populations mean & variance: 
they converge to the correct value as the number of samples increases.

Let's study the distribution of 1000 averages of 40 exponentials. Fig.1 and Fig.2 show the distribution of sample mean & sample variance, plus their mean and theoretical values:

```{r, echo=FALSE}

number_of_samples <- 1000
sample_size <- 40                # 5
expected_var <- 1 / (lambda^2 * sample_size)  # 0.625

# we build our samples pool from seed 1 (to have reproducible data)
set.seed(1); mns0 = NULL; mns_var = NULL
for (i in 1 : number_of_samples) {
  sample <- rexp(sample_size, lambda)
  mns0 = c(mns0, mean(sample))
  mns_var = c(mns_var, var(sample))
}

rm (i,sample)

```

```{r, warning=FALSE, echo=FALSE,  fig.width = 0.8*a4width, fig.height = 0.3*a4height, fig.align='center'}

# plot the samples mean distribution & its mean
p1 = qplot(mns0, geom = 'blank') + 
    geom_histogram(aes(y=..density..), binwidth = 0.1, alpha=0.4) +
    geom_vline (xintercept=mean(mns0), color='salmon2', lwd=1) +
    xlab("mean") +
    ggtitle ("Fig. 1: Sample Mean PDF") +
    theme(title=element_text(size = 8, colour = 'black'),
    plot.title=element_text(face='italic', vjust=2)) +
    annotate("text", x = 8, y = 0.55, hjust = 1, size=2.5,
             label = paste("mean: ", round(mean(mns0),3),
                           "\nexpected mean: ", 1/lambda))

# plot the samples variance distribution & its mean
p2 = qplot(mns_var, geom = 'blank') + 
    geom_histogram(aes(y=..density..), binwidth = 1, alpha=0.4, lwd=1) +
    geom_vline (xintercept=mean(mns_var), color='salmon2', lwd=1) +
    xlab("variance") + xlim(0,100) +
    ggtitle ("Fig. 2: Sample Variance PDF") +
    theme(title=element_text(size = 8, colour = 'black'),
    plot.title=element_text(face='italic', vjust=2)) +
    annotate("text", x = 100, y = 0.05, hjust = 1, size=2.5,
             label = paste("variance mean: ", round(mean(mns_var),3),"\n", "expected variance mean: ", 1/lambda^2))

multiplot(p1, p2, cols=2)
rm (p1,p2,mns_var)

```

We clearly see that **their mean is close to the value they estimate**.

_Please refer to **SI02 - random variables.md** to know more about probability distributions, mean and variance._

\pagebreak

# Asymptotics & Central Limit Theorem

## Definition

**Asymptotics** describes how statistics behave as sample sizes get very large and approach infinity. This is useful for making **statistical inferences** and approximations.

The **Central Limit Theorem** states that, according to the Law of Large Numbers:

> The **distribution $\bar X$ of sample means** of iid obeservations (from a population of mean = $\mu$, sd = $\sigma$) will become **normal**, 
> or nearly normal, as the sample size n increases. It will be approximated by:

> **$\bar X \sim N~(\mu, \sigma^2 / n)$**

An important point is that the CLT works **even if the original distribution is not normal**.

## Example

Fig. 3 shows the PDF for our distribution ($\lambda$=0.2) vs a Normal with the same mean and standard deviation.

Fig.4 shows the empirical distribution of samples mean _(cf. Fig.1)_ VS the CLT predicted one:

+ $Est=1/\lambda = 5$ 
+ $SE_{Est}=1/(\lambda \times \sqrt{n}) = 0.625$


```{r, echo=FALSE, fig.width = 0.8*a4width, fig.height = 0.3*a4height, fig.align='center', warning=FALSE}

# CLT normal
dnorm_CLT <- function (x) {
  y <- dnorm(x, mean = expected_mean, sd = sqrt(expected_var))
  return (y)
}

p1 = qplot(x=c(0,30), geom = 'blank') + 
  stat_function(fun = dexp, arg = list(rate=lambda), aes(colour = 'Exponential'), lwd=1) +
  stat_function(fun = dnorm, arg = list(mean = 1/lambda, sd = 1/lambda), aes(colour = 'Normal'), lwd=1) +
  scale_x_continuous(breaks = seq(0,30,5)) + 
  xlab("X") + ylab("P(X)") + scale_color_discrete(name =element_blank()) +
  ggtitle ("Fig. 3: PDF - Exponential vs Normal") +
  theme(title=element_text(size = 8, colour = 'black'),
        plot.title=element_text(face='italic', vjust=2),
        legend.position='bottom')

# plot the samples distribution & theoretical mean distribution
p2 = qplot(mns0, geom = 'blank') + xlim(2.5,7.5) + ylim(0,0.7) +
  geom_line(aes(y = ..density.., colour = 'Empirical'), stat = 'density', lwd=1) +  
  geom_histogram(aes(y=..density..), binwidth = 0.1, alpha=0.4) +
  stat_function(fun = dnorm_CLT, aes(colour = 'Theoretical'), lwd=1) +
  xlab("mean") +
  scale_colour_discrete(name=element_blank()) +
  ggtitle ("Fig. 4: PDF - Samples Mean vs Normal") +
  theme(title=element_text(size = 8, colour = 'black'),
        plot.title=element_text(face='italic', vjust=2),
        legend.position='bottom')
        
multiplot(p1, p2, cols=2)
rm (p1,p2)

```

We clearly see that:

+ our exponential distribution is **not even close to being normal**
+ **the empirical distribution is very close to being normal**, as predicted by the CLT

\pagebreak

# CLT Confidence Interval

## Definition

As the distribution of sample means $\bar X$ is roughly normal (mean = $\mu$ and sd = $\sigma/\sqrt{n}$),
we have for each of our samples:

$P~(\bar X < mean - 2 sd) = P~(\bar X < \mu - 2 \sigma/\sqrt{n}) \simeq 0.025$

$P~(\bar X > mean + 2 sd) = P~(\bar X > \mu + 2 \sigma/\sqrt{n}) \simeq 0.025$

It means that:

$P~(~\bar X \in [ \mu \pm 2 \sigma/sqrt{n}]~) \simeq 0.95$

```{r, echo=FALSE, fig.width = 0.4*a4width, fig.height = 0.2*a4height, fig.align='center', warning=FALSE}

# Return dnorm(x) for 0 < x < 2, and NA for all other x
int <- 2*sqrt(expected_var)
dnorm_CLT_limit <- function(x) {
  y <- dnorm(x, mean = expected_mean, sd = sqrt(expected_var))
  y[x < expected_mean - int  |  x > expected_mean + int] <- NA
  return(y)
}

# plot the samples distribution & theoretical mean distribution
p1 = qplot(mns0, geom = 'blank') + xlim(2.5,7.5) + ylim(0,0.7) +
  geom_histogram(aes(y=..density..), binwidth = 0.1, alpha=0.4) +
  stat_function(fun = dnorm_CLT, lwd=1) +
  stat_function(fun = dnorm_CLT_limit, geom="area", fill="blue", alpha=0.2) +
  geom_vline (xintercept=expected_mean - int, lwd=0.5, lty=2) +
  geom_vline (xintercept=expected_mean + int, lwd=0.5, lty=2) +
  xlab("mean") +
  scale_colour_discrete(name="Distribution") +
  ggtitle ("Fig. 5: Samples Mean 95% Interval") +
  theme(title=element_text(size = 8, colour = 'black'),
        plot.title=element_text(face='italic', vjust=2))
        
p1

rm (p1,int,dnorm_CLT,dnorm_CLT_limit,mns0,number_of_samples,sample_size)
```

We can deduce from above that:

$P~(~\mu \in [ \bar X \pm 2 \sigma/\sqrt{n}]~) \simeq 0.95$  

\  

$\bar X \pm 2 \sigma/\sqrt{n}$ is called the **95% Confidence Interval** for $\mu$. It mean that for 95% of
our samples, this interval will include $\mu$. But **it does not mean that $\mu$ is actually in this interval**.

+ CI get wider as the coverage increases 
+ CI get narrower as the sample size increases & with less variability

 The confidence interval represents values for the population parameter for which the difference between the parameter 
 and the observed estimate is not statistically significant at the 5% level.

It means that, if the true value of the parameter lies outside the 95% confidence interval once it has been calculated, then an event has occurred which had a probability of 5% (or less) of happening by chance.

## Empirical estimation

The CLT states that: $\bullet~~mean_{Est} \simeq \mu~~\bullet~~SD_{Est} \simeq \sigma~~\bullet~~\bar X \sim N~(\mu, \sigma^2 / n)~~\bullet$ so the CI is:

$$mean_{Est} \pm ZQ_{1-\alpha/2} \times SE_{Est} = mean_{Est} \pm ZQ_{1-\alpha/2} \times \frac{SD_{Est}}{\sqrt{n}}$$


\pagebreak

# T Distribution

## Definition

The CLT works only for large enough sample sizes. For smaller ones, the Gosset's $t$ distribution is
more relevant. It is assumed the population is an iid normal (or roughly symetrical & mound-shaped): 
the t-interval does not work well with skewed data.

In that case:

$$\frac{\bar X - \mu}{S /\sqrt{n}}$$

follows a $t$-distribution with n-1 degrees of freedom _(replacing $S$ by $\sigma$ would give exactly a 
standard normal)_. Its tails are **thicker than normal**, so its Confidence Interval is **wider** for the same Confidence Level.
This is because estimating the population standard deviation introduces more uncertainty.

## Example

Back to the Exponential Distribution, Fig.6 shows the experimental sample distribution vs T vs Normal 
for different sample sizes. The $t$-distribution gets close to normal even for relatively small sample sizes.


```{r, echo=FALSE}

number_of_samples <- 1000
sample_size <- c(2,5,10,20)

# we build our samples pool from seed 1 (to have reproducible data)
set.seed(1); mns1 = NULL;
for (i in 1 : number_of_samples) {
  sample <- lapply(sample_size, function(x) {rexp(x, lambda)}) # exp dist on several sample sizes
  spmean <- sapply(sample, mean)  # vector of means
  spsd <- sapply(sample, sd)      # vector of sd
  tvalue <- (spmean - expected_mean)/(spsd/sqrt(sample_size))
  tvalue <- cbind(tvalue, sample_size)  # matrix sample mean/sample size
  mns1 <- rbind(mns1, tvalue)
}

mnsDf <- as.data.frame(mns1)
colnames (mnsDf) <- c('tvalue', 'size')
mnsDf$size <- factor(mnsDf$size)

# we build our t-distribution curves
x <- seq(-4,4,len=100)
tdist <- data.frame(x=rep(x,each=length(sample_size)),sample_size)
tdist$y <- with(tdist,dt(x,sample_size-1))
colnames (tdist) <- c('x', 'size', 'y')
tdist$size <- factor(tdist$size)

rm (i,mns1,sample,spmean,spsd)

```

```{r, echo=FALSE, fig.width = 0.65*a4width, fig.height = 0.45*a4height, fig.align='center', warning=FALSE}

# plot the samples distribution (converted to T-like) & normal & T

p1 = ggplot(mnsDf, aes(x=tvalue)) + xlim(-4,4) + 
     geom_line(aes(y = ..density.., colour = 'Empirical'), stat = 'density', lwd=0.7) +
     geom_histogram(aes(y=..density..), binwidth = 0.2, alpha=0.2) +
     stat_function(fun = dnorm, aes(colour = 'Normal'), lwd=0.7) +
     geom_line(data=tdist, aes(x,y, colour='T'), lwd=0.7) +
     facet_wrap( ~ size) +
     ggtitle ("Fig. 6: PDF - Samples Mean vs Normal vs T") +
     scale_colour_discrete(name=element_blank()) +
     theme(title=element_text(size = 8, colour = 'black'),
     plot.title=element_text(face='italic', vjust=2),
     legend.position="bottom")

p1

rm (p1,x,number_of_samples,sample_size,mnsDf,tdist,tvalue)

```

\pagebreak

# T Confidence Intervals

## Definition

T Confidence Intervals are slightly different from CLT ones:

$$mean_{Est} \pm TQ_{1-\alpha/2, n-1} \times SE_{Est} = mean_{Est} \pm TQ_{1-\alpha/2, n-1} \times \frac{SD_{Est}}{\sqrt{n}}$$


\  

## Comparizon of CLT vs T Confidence Intervals

Back to the Exponential Distribution, Fig.7 shows how CLT and T Confidence Intervals perform 
for different sample sizes.

```{r, echo=FALSE, warning=FALSE}

number_of_samples <- 1000
sample_size <- c(2,5,10,20, 40, 100, 1000)

# we build our samples pool from seed 1 (to have reproducible data)
set.seed(1); mns2 = NULL;
for (i in 1 : number_of_samples) {
  sample <- lapply(sample_size, function(x) {rexp(x, lambda)}) # exp dist on several sample sizes
  spmean <- sapply(sample, mean)                   # vector of means
  serror <- sapply(sample, sd)/sqrt(sample_size)   # vector of sd errors

  llz <- spmean - qnorm(0.975) * serror
  ulz <- spmean + qnorm(0.975) * serror
  isz <- llz < expected_mean & expected_mean < ulz

  llt <- spmean - qt(0.975, sample_size-1) * serror
  ult <- spmean + qt(0.975, sample_size-1) * serror
  ist <- llt < expected_mean & expected_mean < ult
  
  sp_value <- cbind (sample_size, spmean, isz, ist)
  mns2 <- rbind(mns2, sp_value)
}

# we convert our data in a data frame
mnsDf <- as.data.frame(mns2)
colnames (mnsDf) <- c('size', 'spmean', 'zvalue', 'tvalue')

# we calculate the % of samples for which the 95% CI actually includes the mean
mnsDfGroup <- group_by (mnsDf, size) %>%
              summarize (dist_mean = mean(spmean), dist_se = sd(spmean), zvalue = mean(zvalue), tvalue = mean(tvalue)) %>%
              mutate (zConf=paste('[', round(dist_mean - qnorm(0.975)*dist_se,3), 
                                  ',', round(dist_mean + qnorm(0.975)*dist_se,3), ']'),
                      tConf=paste('[', round(dist_mean - qt(0.975, size-1)*dist_se,3), 
                                  ',', round(dist_mean + qt(0.975, size-1)*dist_se,3), ']'))

# we melt our df to ggplot more easily
mnsDfGroupMelt <- melt (select (mnsDfGroup, size, zvalue, tvalue), id.vars=c('size'))
mnsDfGroupMelt$size <- factor(mnsDfGroupMelt$size)

# we show our results
print.data.frame(select (mnsDfGroup, size, zConf, tConf), row.names=FALSE, include.rownames = FALSE)

rm (i,sample_size,number_of_samples,sample,spmean,serror,llz,ulz,isz,llt,ult,ist,sp_value,mns2,mnsDf,mnsDfGroup)

```

\  

```{r, echo=FALSE, fig.width = 0.5*a4width, fig.height = 0.3*a4height, fig.align='center', warning=FALSE}

# plot the samples distribution (converted to T-like) & normal & T

p1 = ggplot(mnsDfGroupMelt, aes(x=size, y=value, group=variable, colour=variable)) + geom_line(lwd=1) + 
     xlab('Sample Size') + ylab('Percentage of CI that include the mean') + ylim(0.5,1) +
     geom_hline(yintercept = 0.95) +
     #geom_line(aes(y = ..density.., colour = 'Empirical'), stat = 'density', lwd=1) +
     #geom_histogram(aes(y=..density..), binwidth = 0.2, alpha=0.2) +
     #stat_function(fun = dnorm, aes(colour = 'Normal'), lwd=1) +
     #geom_line(data=tdist, aes(x,y, colour='T'), lwd=1) +
     ggtitle ("Fig. 7: 95% Confidence Interval - Normal vs T") +
     scale_colour_discrete(name="Distribution") +
     theme(title=element_text(size = 8, colour = 'black'),
     plot.title=element_text(face='italic', vjust=2))

p1

rm(p1,expected_mean,expected_var,lambda,mnsDfGroupMelt)

```

The $T$-interval is, as expected, **much more reliable for small sample sizes**. The behaviour of the two methods converge when the sample size increases.

\pagebreak

# Comparing two populations

## Definition

$T$-intervals are very useful to compare two populations. 

> Confidence intervals of difference between populations that **do not contain 0** imply 
> that there is a **statistically significant difference** between the populations.

## Example

A classic example is the sleep data analyzed in Gosset's Biometrika paper.
Fig.8 shows the increase in hours of sleep for 10 patients on two soporific drugs: 

```{r, echo=FALSE, fig.width = 0.5*a4width, fig.height = 0.3*a4height, fig.align='center', warning=FALSE}

data(sleep)

# add slope for each ID
sleep$group <- factor(sleep$group )
sleep <- sleep %>% group_by(ID) %>%
         mutate(slope = (extra[group==2] - extra[group==1])/(2-1))


# plot
p1 = ggplot(sleep, aes(x=group, y=extra, group=ID, colour=ID)) + xlab("Drug N°") + ylab("Extra hours of sleep") +
     geom_point() + geom_line() +
     ggtitle ("Fig. 8: Increase in hours of sleep - drug N°1 vs N°2") +
     theme(title=element_text(size = 8, colour = 'black'),
     plot.title=element_text(face='italic', vjust=2)) +
     scale_colour_discrete(name="Patient ID") +
     scale_size(guide = FALSE)

p1

rm (p1)

```

It seems that drug N°2 is more efficient than drug N°1. To confirm this hypothesis, we can take a $t$-test
to calculate the T Confidence Interval of their difference.

```{r}

g1 <- sleep$extra[sleep$group==1]; g2 <- sleep$extra[sleep$group==2]
ttest <-t.test(g2, g1, paired = TRUE)
result <- as.data.frame (cbind(round(ttest$conf, 3)[1], 
                               round(ttest$conf, 3)[2], 
                               round(ttest$p.value, 5)))
names(result) <- c("Lconf", "Uconf", "p.value")
print(result)

```

The T Confidence Interval does not include 0 and P < 0.005, so **drug N°2 is statistically more efficient than
drug N°1**: with 95% probability, the average difference of effects for an individual patient is between .7 and 2.46 additional hours of sleep.

\pagebreak

## Generalization

A generalization of the $t$-test can be used for comparing independant groups (different sample sizes, etc.) with or 
without equal variance. Performing it on Gosset's sleep data gives the following results:

```{r, echo=FALSE}

# create an empty data frame  
df <- data.frame(paired=logical(),
                 eqVar=logical(),
                 Lconf=numeric(),
                 Uconf=numeric(),
                 p.value=numeric()) 

for (myTest in list(c(TRUE,TRUE), c(FALSE,TRUE), c(FALSE,FALSE))) {
  ttest <- t.test(g2, g1, paired = myTest[1], var.equal=myTest[2])
  result <- as.data.frame (cbind(myTest[1],
                                 myTest[2],
                                 round(ttest$conf, 3)[1], 
                                 round(ttest$conf, 3)[2], 
                                 round(ttest$p.value, 5)))
  
  df <- rbind(df, result)
}

names(df) <- c("Paired", "EqVar", "Lconf", "Uconf", "p.value")
print(df)

rm (result,df,g1,g2,ttest,myTest)

```

By omitting the information that the two populations are paired, the results become less clear
(but equal & unequal variance give a very similar CI). We can easily see why by studing the two distributions, as shown
Fig.9.

```{r, echo=FALSE, fig.width = 0.4*a4width, fig.height = 0.3*a4height, fig.align='center', warning=FALSE}

# plot
p1 = ggplot(sleep, aes(x=group, y=extra,fill=group)) +
     geom_boxplot() + 
     #scale_x_discrete(labels=c("0.50", "1.00", "2.00")) +
     ggtitle ("Fig. 9:  Increase in hours of sleep \ndrug N°1 vs N°2 as independent groups") +
     theme(title=element_text(size = 8, colour = 'black'),
           plot.title=element_text(face='italic', vjust=2),
           axis.title.x=element_blank())

p1

rm(p1,sleep)

```

\pagebreak

# Hypothesis Testing

## Principle

Hypothesis testing is the use of statistics to determine the **probability that a given hypothesis is true**. It is used to make decisions about populations using observed data.

The usual process of hypothesis testing consists of four steps:

1. **Formulate** the null hypothesis $H_0$ and the alternative hypothesis $H_a$. Commonly:

    + $H_0$: the observations are the result of pure chance
    + $H_a$: the observations show a real effect combined with a component of chance variation
    + **Statistical evidence** is required to reject H_0 in favor of the alternative hypothesis

2. Identify a **test statistic** that can be used to assess the truth of H_0. 

3. **Compute** the P-value. The **smaller** the P-value, the **stronger** the evidence **against $H_0$**.

4. **Compare** the P-value to an acceptable significance value $\alpha$. If $P\leq\alpha$:

    +  the observed effect is **statistically significant**
    +  the null hypothesis is ruled out, and the **alternative hypothesis** is **valid**

## Outcomes

There are four possible outcomes of our statistical decision process:

\  

Truth | Decide | Result |
:---:|:---:|:---|
$H_0$ | $H_0$ | Correctly accept null |
$H_0$ | $H_a$ | Type I error $\alpha$ |
$H_a$ | $H_a$ | Correctly reject null |
$H_a$ | $H_0$ | Type II error $\beta$ |

+ Type I error: REJECTS a TRUE null hypothesis $H_0$
+ Type II error: ACCEPTS a FALSE null hypothesis $H_0$

Since there's some element of **uncertainty** in questions concerning populations, we deal with **probabilities**. In our hypothesis testing we'll set the probability of **making errors small**.

The probabilities of making these two kinds of errors are related. If you decrease the probability of making a Type I error (rejecting a true hypothesis), 
you increase the probability of making a Type II error (accepting a false one) and vice versa.

## P-value and Alpha

> The **P-value** is the probability that a test statistic at least as significant as the one observed 
> would be obtained if $H_0$ were true.

We reject $H_0$ when P < $\alpha$. It means that $\alpha$ is the **Type I error rate** or **level** 3= Probability of rejecting $H_0$ when it is correct.


\pagebreak

## Example: sample mean

Let's suppose we have a sample of mean = $\bar X$ and standard deviation $S$. Our hypothesis $H_0$ is that the mean
of the population from which our sample is drawn is $\mu_0$:

$H_0:\mu = \mu_0$

Under $H_0$, the sample mean distribution $Est \sim N~(\mu_0,S/\sqrt{n})$. We want to measure how far from $\mu_0$ our
sample mean is: if it is too far away to be statistically likely, we can reasonably reject $H_0$. Otherwise, we will **fail 
to reject** $H_0$.

To challenge $H_0$, we will consider the Test Statistic:

$$TS = \frac{\bar{X} - \mu_0}{S/\sqrt{n}}$$

This reduces the Hypothesis Testing to the following table, where the TS is called the **Z-score**:

\  

Alternate Hyp.   | Reject $H_0$ if |
:---------------:|:----------------|
$\mu < \mu_0$    |$TS \leq Z_{\alpha} = -Z_{1 - \alpha}$
$\mu \neq \mu_0$ |$|TS| \geq Z_{1 - \alpha / 2}$
$\mu > \mu_0$    |$TS \geq Z_{1 - \alpha}$


For small sample sizes, the $T$-test is performed the same way:

Alternate Hyp.   | Reject $H_0$ if |
:---------------:|:----------------|
$\mu < \mu_0$    |$TS \leq t_{\alpha, n-1} = -t_{1 - \alpha, n-1}$
$\mu \neq \mu_0$ |$|TS| \geq t_{1 - \alpha / 2, n-1}$
$\mu > \mu_0$    |$TS \geq t_{1 - \alpha, n-1}$

\  

## Link with Confidence Interval

When we test $H_0: \mu = \mu_0$ versus $H_a: \mu \neq \mu_0$, we fail to reject $H_0$ for all values $\bar X$
where $TS \leq Z_{1 - \alpha / 2}$. This set is a $(1-\alpha)100\%$ confidence interval for $\mu$.

The same works in reverse; if a $(1-\alpha)100\%$ interval contains $\mu_0$, then we **fail  to reject** $H_0$.

\pagebreak

# Power

## Definition

A type II error is **failing to reject $H_0$ when it's false**. Its probability is usually called $\beta$.

Its opposite is the **probability of rejecting $H_0$ when it is false**. It is called **power**: $power = 1- \beta$

Reminder: $\alpha$ is the **probability of rejecting $H_0$ when it is correct**.

Power comes into play when you're designing an experiment, and in particular, if you're trying to determine **if a null result**
(failing to reject a null hypothesis) is **meaningful**.

For instance, you might have to determine if your sample size was big enough to yield a meaningful, rather than random, result.

## Link with Type I Error

Let's consider the hypothesis $H_a: \mu = \mu_a > \mu_0$. In that case **(same for $t$-tests)**:

$$\alpha = P\left(TS > Z_{1-\alpha} ~;~ \mu = \mu_0 \right)~~\bullet~~Power = P\left(TS > Z_{1-\alpha} ~;~ \mu = \mu_a \right)$$

Power increases as:

+ $\alpha$ increases
+ n gets larger
+ $\mu_a$ gets further away from $\mu_0$
+ S decreases

\  

If $H_a: mu \neq mu_0$ we would calculate the one sided power using alpha / 2 in the direction of mu_a (This is only
approximately right, it excludes the probability of getting a large test statistic in the opposite direction of the truth).
As $\alpha$ is bigger than $\alpha/2$, power of a one-sided test is greater than the power of the associated two sided test.

## Calculating Power

The quantities $mu_0$ and $\alpha$ are specified by the test designer. The other four quantities ($\beta$, $\sigma$, $n$, and $\mu_a$), are all unknown. 
Knowing three of these, we can solve for the missing fourth: usually $power = 1 -\beta$ or the sample size $n$.

Note that:

+ only the TS $\sqrt{n}*(\mu_a - \mu_0) /\sigma$ is needed
+ $(\mu_a - \mu_0) /\sigma$ is called the **effect size**: the difference in the means in standard deviation units.
+ the effect size is **unit free**, so it can be interpreted in different settings.

We can calculate the Power in R: 

```{r eval=FALSE}
power.t.test (n = sampleSize, delta = mu_a - mu_0, sd = sigma, ...)$power # power for a fixed setting
power.t.test (power = requiredPower, delta = mu_a - mu_0, sd = sigma, ...)$n # n for specific power
```

\pagebreak

## Example

Fig.10-11 show an example of $H_a: \mu_a < \mu_0$ and an example of $H_a: \mu_a > \mu_0$. The vertical black bar 
shows the measured mu value below or above which we can statistically reject $H_0$ ($\alpha = 0.05$), and the corresponding
power (that depends on the value of $\mu_a$).

\  

```{r,echo=FALSE,warning=FALSE}

mu0 = 30 
myplot <- function(sigma, mua, n, alpha, type) {
  
  # calculate parameters
  xitc = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)
  yitc = mu0 - qnorm(1 - alpha) * sigma/sqrt(n)
  muL = min(mu0,mua)
  muU = max(mu0,mua)
  mumin = muL - 6; mumax = muU + 6

  # calculate points under the normals
  df <- dplyr::data_frame(
          x = seq(mumin, mumax, length = 1000),
          alphaL = ifelse(x < yitc, dnorm(x, mean = mu0, sd = sigma/sqrt(n)), NA),
          powerL = ifelse(x < yitc, dnorm(x, mean = mua, sd = sigma/sqrt(n)), NA),
          alphaG = ifelse(x > xitc, dnorm(x, mean = mu0, sd = sigma/sqrt(n)), NA),
          powerG = ifelse(x > xitc, dnorm(x, mean = mua, sd = sigma/sqrt(n)), NA)
  )
  
  g = ggplot(df, aes(x)) + xlab("mu")

  # draw normals of mean mu0 and mua
  g = g + stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma/sqrt(n)), aes(colour='mu0'), lwd = 1)
  g = g + stat_function(fun = dnorm, args = list(mean = mua, sd = sigma/sqrt(n)), aes(colour='muA'), lwd = 1)
  
  # draw area under the normals, depending on the comp type
  if (type=='G') {
    g = g + geom_area(aes(y = alphaG, fill="alpha"), alpha=0.4, na.rm = TRUE)
    g = g + geom_area(aes(y = powerG, fill="power"), alpha=0.2, na.rm = TRUE)
    g = g + geom_vline(xintercept = xitc, size = 1)
    z = qnorm(1 - alpha) 
    alphaP = pnorm(xitc, mean = mu0, sd = sigma/sqrt(n), lower.tail = FALSE)
    powerP = pnorm(xitc, mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)
  }
  else {
    g = g + geom_area(aes(y = alphaL, fill="alpha"), alpha=0.4, na.rm = TRUE)
    g = g + geom_area(aes(y = powerL, fill="power"), alpha=0.2, na.rm = TRUE)
    g = g + geom_vline(xintercept = yitc, size = 1)
    alphaP = pnorm(yitc, mean = mu0, sd = sigma/sqrt(n))
    powerP = pnorm(yitc, mean = mua, sd = sigma/sqrt(n))
  }
  
  # calculate power & alpha probabilities
  g = g + annotate("text", x = mumax, y = 0.25, hjust = 1, size=2.5,
             label = paste("alpha: ", round(alphaP,3),
                           "\npower: ", round(powerP,3)))
  
  g = g + scale_colour_discrete(name="Hypothesis") + scale_fill_discrete(name="Probability")
  g = g + theme(title=element_text(size = 8, colour = 'black'),
                plot.title=element_text(face='italic', vjust=2),
                legend.position='bottom',
                axis.title.y=element_blank())
  g
}

p1 = myplot(6,26,16,0.05,'L')
p1 = p1 + ggtitle ("Fig. 10: alpha vs power; muA = mu0 - 4 ")

p2 = myplot(6,32,16,0.05,'G')
p2 = p2 + ggtitle ("Fig. 11: alpha vs power; mu1 = mu0 + 2 ")

multiplot(p1, p2, cols=2)
rm (p1,p2)

#manipulate(myplot(sigma, mua, n, alpha),
#           sigma = slider(1, 10, step = 1, initial = 4),
#           mua = slider(30, 35, step = 1, initial = 32),
#           n = slider(1, 50, step = 1, initial = 16),
#           alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05))

rm (mu0,myplot) # comment out when using manipulate

```


\pagebreak

# Multiple Testings

Multiple testing is particularly relevant now in this age of BIG data. Statisticians are tasked with questions such as:

+ Which variables matter among the thousands measured?
+ How do you relate unrelated information?

When doing a large number of tests, even small Type I/Type II Error Rates can lead to many erroneous results. Technics exist to
limit this.


## Type of errors

When performing m tests on $H_0$ vs $H_a$, we have the following outcomes:


  Test Result    | Chosen Hyp    | $H_0$ is true | $H_a$ is true | N° of results  | Accuracy Rate
 :--------------:|:-------------:|:-------------:|:-------------:|:--------------:|:-----------------------
  $P > \alpha$ _(negative)_  | $H_0$         | U             | T [$\beta$]   | $m - R$        | Specificity: $U/(m-R)$
  $P < \alpha$ _(positive)_  | $H_a$         | V [$\alpha$]  | S             | $R$            | Sensitivity: $S/R$
  --             | N° of tests   |$m_0$          | $m - m_0$     | $m$            | 


+ Type I error _(false positive)_: V results. Say $H_0$ is false when it is not 
+ Type II error _(false negative)_: T results. Say $H_0$ if true when it is not

_Note: we reject $H_0$ when outcomes are considered significant (ie. positive), ie P < $\alpha$._

## Error rates

__False positive rate__ - The rate at which false results (falsely rejecting $H_0$) are called significant: $E\left[\frac{V}{m_0}\right]$

__Family wise error rate (FWER)__ - The probability of at least one false positive ${\rm Pr}(V \geq 1)$

__False discovery rate (FDR)__ - The rate at which claims of significance are false: $E\left[\frac{V}{R}\right]$


## Control FWER: Bonferroni correction

Idea: $Pr(V \geq 1) < \alpha$ 

+ Set $\alpha_{fwer} = \alpha/m$
+ Call any  $P_{(i)} \leq \alpha_{fwer}$ significant

__Pros__: conservative
__Cons__: May be very conservative


## Controlling FDR: Benjamini-Hochberg method (BH)

Idea: $E\left[\frac{V}{R}\right] < \alpha$ 

+ Order the P-values from smallest to largest $P_{(1)},...,P_{(m)}$
+ Call any $P_{(i)} \leq \alpha \times \frac{i}{m}$ significant

__Pros__: less conservative (maybe much less)
__Cons__: more false positives, may behave strangely under dependence

\pagebreak

## Example

```{r}
# 50% true positive: y = f(x)
set.seed(1010093) 
pValues <- rep(NA, 1000) 
for (i in 1:1000) {    
  x <- rnorm(20)    
  # First 500 beta=0, last 500 beta=2  (regression slope) 
  if (i <= 500) { y <- rnorm(20) }
  else { y <- rnorm(20, mean = 2 * x) }
  pValues[i] <- summary(lm(y ~ x))$coeff[2, 4]
} 

# not zero = significant; We get 24 False positive - around 5%
trueStatus <- rep(c("zero", "not zero"), each = 500)
table(pValues < 0.05, trueStatus)

# Controls FWER - no more false positive, but the threshold is so low we now have 23 false negative
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)

# Controls FDR - less false positive, no false negative
table(p.adjust(pValues, method = "BH") < 0.05, trueStatus)
```

\pagebreak

# Resampling

## Bootstrapping

Bootstrapping is a technique that uses simulation and computation to infer **distributional properties** you might not otherwise be able to determine.

Its basic principle is to use OBSERVED data as a **substitute for the population**. We can simulate observations by doing **random sampling with replacement**. 
From this distribution (constructed from the observed data), we can **estimate the distribution** of the statistic we're interested in. This lets us better
understand the underlying population (from which we didn't have enough data).

We can easily do it in R _(see also the R package bootstrap)_:

```{r eval=FALSE}
# create a matrix of N new samples of the same size as obsData
newMatrix <- matrix (sample (obsData, obsSize*N,replace=TRUE), N, obsSize)

# calculate the statistic value of each new sample (here, the median)
statVector <- apply (newMatrix, 1, median)

# calculate a CI for the resulting statistic
quantile(statVector, c(.025,.975))
```


## Permutation testing

Permutation testins is also based on resampling a single dataset, but it measures whether or not outcomes are independent of group identity 
(ie. if group labels are exchangable). The resampling simply permute group labels associated with outcomes.

The idea is to see if the dataset value is statistically likely when the outcomes are independent of group identity. 
To do so, we can estimate the statistic distribution by resampling, and calculate P.
